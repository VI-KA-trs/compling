{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VI-KA-trs/compling/blob/main/Barinova_hw_tokenization_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re # импорт модуля регулярных выражений\n",
        "def tokenization_space_punc(full_text): # запись функции для токенизации текста по пробелам и знакам препинания\n",
        "  tokens = re.findall(r'\\w+', full_text) # формирует переменную, в которую записываются найденные регулярные выражение: последовательность символов слова (т.е. токенизация по пробелам и знакам препинания)\n",
        "  return tokens # возвращает токенизированный текст в формате списка, где каждый токен - элемент списка\n"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk # импорт библиотеки NLTK для обработки естественного языка\n",
        "nltk.download('punkt')  # загрузка модели токенизации 'punkt', необходимой для работы word_tokenize\n",
        "nltk.download('punkt_tab') # загрузка таблицы токенизации 'punkt_tab' (дополнение для корректной работы токенизатора)\n",
        "\n",
        "def tokenization_nltk(full_text): # определение функции токенизации текста с помощью библиотеки NLTK\n",
        "  from nltk.tokenize import word_tokenize # импорт функции word_tokenize из модуля tokenize\n",
        "  return word_tokenize(full_text)   # возвращает список, где каждый элемент — отдельный токен"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ecc18b-2258-472e-a31d-ddf3c9172055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy # импорт библиотеки spaCy\n",
        "def tokenization_spacy(full_text): # определение функции токенизации текста с помощью spaCy\n",
        "  nlp = spacy.load(\"en_core_web_sm\") # загрузка английской языковой модели spaCy\n",
        "  doc = nlp(full_text)  # обработка текста моделью spaCy (создаётся объект Doc, содержащий токены)\n",
        "  result = [t.text for t in doc] # формирование списка токенов: извлекаются текстовые значения каждого токена из объекта doc\n",
        "  return result # возвращает список, где каждый элемент — отдельный токен"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd14b25-d38b-4b49-bce0-2c6feff60be9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Токенизация по пробелам и знакам препинания: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'It', 's', 'a', 'beautiful', 'day']\n",
            " \n",
            "Токенизация с помощью NLTK: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            " \n",
            "Токенизация с помощью spyCy: ['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!']\n",
            "------------------------------------------------------------\n",
            "Токенизация по пробелам и знакам препинания: ['Dr', 'Smith', 'arrived', 'at', '5', '30', 'p', 'm', 'from', 'New', 'York', 'The', 'meeting', 'cost', '1', '000', '50']\n",
            " \n",
            "Токенизация с помощью NLTK: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            " \n",
            "Токенизация с помощью spyCy: ['Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.']\n",
            "------------------------------------------------------------\n",
            "Токенизация по пробелам и знакам препинания: ['I', 'can', 't', 'believe', 'she', 's', 'going', 'Let', 's', 'meet', 'at', 'Jane', 's', 'house', 'They', 'll', 'love', 'it']\n",
            " \n",
            "Токенизация с помощью NLTK: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            " \n",
            "Токенизация с помощью spyCy: ['I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.']\n",
            "------------------------------------------------------------\n",
            "Токенизация по пробелам и знакам препинания: ['What', 's', 'the', 'ETA', 'for', 'the', 'package', 'Please', 'e', 'mail', 'support', 'example', 'com', 'ASAP']\n",
            " \n",
            "Токенизация с помощью NLTK: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            " \n",
            "Токенизация с помощью spyCy: ['What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for i in text: # перебор элементов списка text\n",
        "  print(f'Токенизация по пробелам и знакам препинания: {tokenization_space_punc(i)}') # вывод результата токенизации по пробелам и знакам пунктуации\n",
        "  print(' ') # элемент оформления\n",
        "  print(f'Токенизация с помощью NLTK: {tokenization_nltk(i)}') # вывод результата токенизации с помощью библиотеки NLTK\n",
        "  print(' ') # элемент оформления\n",
        "  print(f'Токенизация с помощью spyCy: {tokenization_spacy(i)}')  # вывод результата токенизации с помощью библиотеки spaCy\n",
        "  print(\"---\"*20) # элемент оформления"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)"
      ],
      "metadata": {
        "id": "mgE2bQFXv0MG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Разделение по пробелам и знакам препинания оказывается недостаточной для современных NLP-задач. Так, например, при пословной токенизации могут встречаться слова, отсутствующие в словаре (проблема Out-of-Vocabulary), в таком случае слово приходится тэгировать как неизвестное. Например, такая проблема может встречаться в нейронных моделях перевода, когда слово, отсутствующее в словаре (предположим, редкое слово \"чакалака\") будет заменено на тэг \"unk\". Кроме того, иногда смысловое деление для конкретных задач требует более детально ориентированную токенизацию. Например, инициалы + фамилия представляют одну смысловую единицу, хотя простая пословная токенизация выделит два токена: \"В.Д.\", \"Баринова\". Более того, существуют языки, в которых отсутствуют пробелы (например, китайский), поэтому пословная токенизация для них невозможна. Также, такой тип токенизации нечувствителен к морфологии (для морфологически богатых языков), где отдельные морфемы также могут представлять собой отдельные смысловые единицы. Так, в слове \"некрасивый\" имеет смысл выделять токен \"не\", представляющий собой отрицательную приставку, которая может модерировать смысл и тональность слова.\n",
        "2. Для получения количества токенов во фразе в модели GPT-5 использовался сервис https://gpt-tokenizer.dev/ Ресурс оценил количество токенов, выделяемых во фразе запрашиваемой моделью, в **10 токенов**.\n",
        "3. Алгоритм токенизации Byte Pair Encoding опирается на статистическое выделение токенов и выполняется в несколько этапов. На первых этапах текст представлен в виде отдельных символов, где конец слова отмечается отдельным маркером /w. Например (\"а на нас ананас\"): а /w н а /w н а с /w а н а н а с /w. На следующем этапе происходит подсчет каждой встретившейся пары символов. Так: 1. пара - а н, 2 раза; 2. пара - н а, 4 раза; 3. пара -  а с, 2 раза; 4. пара: а /w, 2 раза; 5. пара: с /w, 2 раза. Наиболее частотная пара на следующем этапе кодируется как один символ, впоследствии формируются новые пары, для которых рассчитываются частотности. В нашем случае н а кодируется как один символ, образуя следующие пары: 1. пара - а /w, 1 раз; 2. пара - на /w, 1 раз; 3. пара - на с, 2 раза; 4. пара с /w, 2 раза; 5. пара а на, 1 раз; 6. пара - на на, 1 раз. Затем процедура повторяется кодированием наиболее частотных пар в отдельный символ и подсчетом новых частот. Операция выполняется до максимально возможного числа повторений. После всех вычислений формируется финальный словарь токенов, рассчитанных по частоте.\n"
      ],
      "metadata": {
        "id": "lUngcxRiV6jw"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}